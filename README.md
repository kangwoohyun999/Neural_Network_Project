# ì‹ ê²½ë§ ì¡°ë³„ê³¼ì œ

## Fashion-mnist ë‹¤ìš´ë¡œë“œ
https://github.com/zalandoresearch/fashion-mnist
* ZIP íŒŒì¼ ë‹¤ìš´ë¡œë“œ í›„ ì••ì¶• í•´ì œ
* data/fashion/... (4ê°œ íŒŒì¼)
* Neural_Network_Project/dataset ì— ì˜®ê¸°ê¸°

## ì‹¤í–‰


## ìµœê³  Accuracyë¥¼ ë„ì¶œí•˜ëŠ” ë°©ë²• (PPT í™œìš© ì˜ˆì •, ìˆ˜ì •ì¤‘)
* Train / Validation ë¶„ë¦¬
* BatchNorm / Dropout ì ìš©í•œ MultiLayerNetExtend ì‚¬ìš©
* ì ì ˆí•œ WeightDecay & Learning-rate schedule ì ìš©
* Epoch ìˆ˜ ì¶©ë¶„íˆ ëŠ˜ë¦¬ê¸° (200~300 epoch)
* Mini-batch SGD + Adam í˜¼í•© or AdamW ì‚¬ìš©
* ì„±ëŠ¥ ì¢‹ì€ layer êµ¬ì„± (128-128-64-64)
* EarlyStopping or Best model ì €ì¥

# Neural_Network_Project

> Python ê¸°ë°˜ ì‹ ê²½ë§ í”„ë¡œì íŠ¸ (Fashion-MNIST)
> Multi-layer Perceptron(MLP) / CNN êµ¬ì¡°ë¥¼ í•™ìŠµí•˜ê³  í‰ê°€í•©ë‹ˆë‹¤.
> Weight Decay, Dropout, BatchNorm ë“± Regularization ê¸°ë²• ì ìš© ê°€ëŠ¥

---

## ğŸ“‚ í”„ë¡œì íŠ¸ êµ¬ì¡° (ìˆ˜ì •ì¤‘)

```
Neural_Network_Project/
 â”œâ”€â”€ dataset/                # Fashion-MNIST ë°ì´í„° ì €ì¥ í´ë”
 â”œâ”€â”€ models/                 # MLP/CNN ëª¨ë¸ ì •ì˜
 â”œâ”€â”€ utils/                  # ë°ì´í„° ì²˜ë¦¬ ë° ìœ í‹¸
 â”œâ”€â”€ train.py                # í•™ìŠµ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
 â”œâ”€â”€ evaluate.py             # í…ŒìŠ¤íŠ¸/ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
 â”œâ”€â”€ plot.py                 # ì†ì‹¤/ì •í™•ë„ ì‹œê°í™”
 â””â”€â”€ README.md
```

* **dataset/**: Fashion-MNIST 4ê°œ íŒŒì¼ì„ ì €ì¥
* **models/**: MLP ë˜ëŠ” CNN ëª¨ë¸ í´ë˜ìŠ¤
* **utils/**: ë°ì´í„° ë¡œë”, ì „ì²˜ë¦¬, ë³´ì¡° í•¨ìˆ˜
* **train.py**: í•™ìŠµ ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸
* **evaluate.py**: í•™ìŠµëœ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
* **plot.py**: í•™ìŠµ ê³¡ì„  ì‹œê°í™”

---

## ğŸš€ ì„¤ì¹˜ ë° ì‹¤í–‰ ë°©ë²•

### 1. ì €ì¥ì†Œ í´ë¡ 

```bash
git clone https://github.com/kangwoohyun999/Neural_Network_Project.git
cd Neural_Network_Project
```

### 2. Python í™˜ê²½ êµ¬ì„±

```bash
pip install -r requirements.txt
```

ë˜ëŠ” ê°œë³„ ì„¤ì¹˜:

```bash
pip install numpy matplotlib torch torchvision
```

### 3. ë°ì´í„° ì¤€ë¹„

Fashion-MNIST ë°ì´í„° ZIP ë‹¤ìš´ë¡œë“œ í›„ ì••ì¶• í•´ì œ â†’ dataset í´ë”ì— 4ê°œ íŒŒì¼ ë³µì‚¬:

```
train-images-idx3-ubyte
train-labels-idx1-ubyte
t10k-images-idx3-ubyte
t10k-labels-idx1-ubyte
```

### 4. í•™ìŠµ ì‹¤í–‰

```bash
python train.py
```

### 5. ì„±ëŠ¥ í‰ê°€

```bash
python evaluate.py
```

---

## ğŸ§  ì½”ë“œ ë™ì‘ íë¦„ (train.py ê¸°ì¤€)

1. **ë°ì´í„° ë¡œë”©**: Fashion-MNIST ë¶ˆëŸ¬ì˜¤ê¸° â†’ Train/Validation ë¶„ë¦¬ â†’ DataLoader êµ¬ì„±
2. **ëª¨ë¸ ìƒì„±**: MLP / CNN ëª¨ë¸ ì´ˆê¸°í™” (He/Xavier Init ê°€ëŠ¥)
3. **ì†ì‹¤ í•¨ìˆ˜ & ì˜µí‹°ë§ˆì´ì €**: CrossEntropyLoss + SGD/Adam/AdamW
4. **í•™ìŠµ ë°˜ë³µ**:

   * Forward â†’ Loss ê³„ì‚°
   * Backward â†’ Gradient ê³„ì‚°
   * Optimizer Step â†’ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
   * Accuracy / Loss ê¸°ë¡
5. **ê²€ì¦**: Epochë§ˆë‹¤ Validation accuracy í™•ì¸
6. **ê²°ê³¼ ì €ì¥**: ëª¨ë¸(.pth) ì €ì¥, plot.pyë¡œ í•™ìŠµ ê³¡ì„  ì‹œê°í™”

---

## âš™ï¸ Weight Decay (ê°€ì¤‘ì¹˜ ê°ì†Œ)

### ê°œë…

* ëª¨ë¸ ê°€ì¤‘ì¹˜ í¬ê¸°ë¥¼ ì¤„ì´ëŠ” regularization
* ê³¼ì í•© ë°©ì§€ ë° ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ

### ì ìš© ë°©ë²• 1: AdamW

```python
optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)
```

### ì ìš© ë°©ë²• 2: SGD + weight_decay

```python
optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)
```

### í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ (ì˜µì…˜)

```python
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)
```

---

## ğŸ“ˆ ê¸°íƒ€ ê¶Œì¥ ì„¤ì •

| í•­ëª©                  | ì„¤ëª…               |
| ------------------- | ---------------- |
| Dropout             | ê³¼ì í•© ë°©ì§€           |
| Batch Normalization | í•™ìŠµ ì•ˆì •ì„± ì¦ê°€        |
| Xavier / He Init    | ê¹Šì€ ë„¤íŠ¸ì›Œí¬ í•™ìŠµ ì†ë„ ê°œì„  |
| Early Stopping      | ë¶ˆí•„ìš”í•œ epoch í•™ìŠµ ë°©ì§€ |
| Train/Val split     | ê³¼ì í•© ëª¨ë‹ˆí„°ë§ í•„ìˆ˜      |

---

## ğŸ“œ ì˜ˆì‹œ í•™ìŠµ ì½”ë“œ (Weight Decay í¬í•¨)

```python
import torch
from models.mlp import MLP
from utils.dataset import load_fashion_mnist

# 1. ë°ì´í„°
train_loader, val_loader = load_fashion_mnist(batch_size=64)

# 2. ëª¨ë¸
model = MLP()
model.to("cuda")

# 3. Optimizer with Weight Decay
optimizer = torch.optim.A
```
