# ì‹ ê²½ë§ ì¡°ë³„ê³¼ì œ

## pkl íŒŒì¼ì€ ì˜ˆì‹œ íŒŒì¼ì…ë‹ˆë‹¤. ìƒì„±ëœ íŒŒì¼ ì•„ë‹™ë‹ˆë‹¤.

## ì‹¤í–‰ ë°©ë²•
ex) C:\Users\user\Downloads\Neural_Network_Project\dataset\mnist.py
* mnist.py ì‹¤í–‰
* (í…ŒìŠ¤íŠ¸ì¤‘)

## Fashion-mnist ë‹¤ìš´ë¡œë“œ (mnist ì‹¤í–‰í•˜ë©´ ì•Œì•„ì„œ ì„¤ì¹˜ë¨)
https://github.com/zalandoresearch/fashion-mnist
* ZIP íŒŒì¼ ë‹¤ìš´ë¡œë“œ í›„ ì••ì¶• í•´ì œ
* data/fashion/... (4ê°œ íŒŒì¼)
* Neural_Network_Project/dataset ì— ì˜®ê¸°ê¸°

## ì‹¤í–‰


## ğŸ¯ ê°„ë‹¨ ì†Œê°œ

1. ëª©ì 

* Fashion-MNIST ë¶„ë¥˜
* êµì¬ 4â€“6ì¥ ê¸°ë°˜
* Adam ì‚¬ìš©
* 6ì¸µ ì´í•˜ ì‹ ê²½ë§

2. ë°ì´í„°ì…‹ ì„¤ëª…

* 28Ã—28 gray, 10 classes
* Train 60,000 / Test 10,000

3. ëª¨ë¸ êµ¬ì¡°

* Input 784
* Dense 256 â†’ ReLU
* Dense 256 â†’ ReLU
* Dense 128 â†’ ReLU
* Output 10

4. ë°©ë²•ë¡ 

* Adam â†’ ë¹ ë¥¸ ìˆ˜ë ´
* Dropout â†’ ê³¼ì í•© ì™„í™”
* ReLU + He ì´ˆê¸°í™”
* ë¯¸ë‹ˆë°°ì¹˜ í•™ìŠµ

5. ì‹¤í—˜

* Learning rate ë¹„êµ
* Dropout ìœ ë¬´ ë¹„êµ
* Batchnorm ìœ ë¬´ ë¹„êµ

6. ê²°ê³¼

* ìµœì¢… Train/Test accuracy
* Loss / Accuracy curve

7. ê²°ë¡ 

* Dropoutì´ ê°€ì¥ íš¨ê³¼ì 
* Adamì´ SGD ëŒ€ë¹„ ë¹ ë¦„

---

## ğŸ“‚ í”„ë¡œì íŠ¸ êµ¬ì¡° (ì•„ë˜ ì „ë¶€ ë‹¤ ìˆ˜ì •ì¤‘)

```
Neural_Network_Project
 â”‚
 â”œâ”€â”€ common
 â”‚    â”œâ”€ __init__.py
 â”‚    â”œâ”€ functions.py
 â”‚    â”œâ”€ layers.py
 â”‚    â”œâ”€ optimizer.py
 â”‚    â”œâ”€ util.py
 â”‚    â”œâ”€ multi_layer_net.py
 â”‚    â”œâ”€ multi_layer_net_extend.py
 â”‚    â””â”€ gradient.py
 â”‚
 â”œâ”€ data
 â”‚    â””â”€ fashion
 â”‚          â”œâ”€ t10k-images-idx3-ubyte.gz
 â”‚          â”œâ”€ t10k-labels-idx1-ubyte.gz
 â”‚          â”œâ”€ train-images-idx3-ubyte.gz
 â”‚          â””â”€ train-labels-idx1-ubyte.gz
 â”‚
 â”œâ”€ utils
 â”‚    â”œâ”€ __init__.py
 â”‚    â”œâ”€ argparser.py
 â”‚    â”œâ”€ helper.py
 â”‚    â””â”€ mnist_reader.py
 â”‚
 â”œâ”€ network_Team7.pkl
 â”œâ”€ activation_init_compare_fashion_mnist.py
 â”œâ”€ depth_compare_fashion_mnist.py
 â”œâ”€ weight_decay_compare_fashion_mnist.py
 â”œâ”€ optimizer_compare_fashion_mnist.py
 â”œâ”€ train_fashion_mnist_team7.py
 â””â”€ README.md
```

---

## ğŸ§  ì½”ë“œ ë™ì‘ íë¦„ (train.py ê¸°ì¤€)

1. **ë°ì´í„° ë¡œë”©**: Fashion-MNIST ë¶ˆëŸ¬ì˜¤ê¸° â†’ Train/Validation ë¶„ë¦¬ â†’ DataLoader êµ¬ì„±
2. **ëª¨ë¸ ìƒì„±**: MLP / CNN ëª¨ë¸ ì´ˆê¸°í™” (He/Xavier Init ê°€ëŠ¥)
3. **ì†ì‹¤ í•¨ìˆ˜ & ì˜µí‹°ë§ˆì´ì €**: CrossEntropyLoss + SGD/Adam/AdamW
4. **í•™ìŠµ ë°˜ë³µ**:

   * Forward â†’ Loss ê³„ì‚°
   * Backward â†’ Gradient ê³„ì‚°
   * Optimizer Step â†’ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
   * Accuracy / Loss ê¸°ë¡
5. **ê²€ì¦**: Epochë§ˆë‹¤ Validation accuracy í™•ì¸
6. **ê²°ê³¼ ì €ì¥**: ëª¨ë¸(.pth) ì €ì¥, plot.pyë¡œ í•™ìŠµ ê³¡ì„  ì‹œê°í™”

---

## âš™ï¸ Weight Decay (ê°€ì¤‘ì¹˜ ê°ì†Œ)

### ê°œë…

* ëª¨ë¸ ê°€ì¤‘ì¹˜ í¬ê¸°ë¥¼ ì¤„ì´ëŠ” regularization
* ê³¼ì í•© ë°©ì§€ ë° ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ

### ì ìš© ë°©ë²• 1: AdamW

```python
optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)
```

### ì ìš© ë°©ë²• 2: SGD + weight_decay

```python
optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)
```

### í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ (ì˜µì…˜)

```python
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)
```

---

## ğŸ“ˆ ê¸°íƒ€ ê¶Œì¥ ì„¤ì •

| í•­ëª©                  | ì„¤ëª…               |
| ------------------- | ---------------- |
| Dropout             | ê³¼ì í•© ë°©ì§€           |
| Batch Normalization | í•™ìŠµ ì•ˆì •ì„± ì¦ê°€        |
| Xavier / He Init    | ê¹Šì€ ë„¤íŠ¸ì›Œí¬ í•™ìŠµ ì†ë„ ê°œì„  |
| Early Stopping      | ë¶ˆí•„ìš”í•œ epoch í•™ìŠµ ë°©ì§€ |
| Train/Val split     | ê³¼ì í•© ëª¨ë‹ˆí„°ë§ í•„ìˆ˜      |

---

## ğŸ“œ ì˜ˆì‹œ í•™ìŠµ ì½”ë“œ (Weight Decay í¬í•¨)

```python
import torch
from models.mlp import MLP
from utils.dataset import load_fashion_mnist

# 1. ë°ì´í„°
train_loader, val_loader = load_fashion_mnist(batch_size=64)

# 2. ëª¨ë¸
model = MLP()
model.to("cuda")

# 3. Optimizer with Weight Decay
optimizer = torch.optim.A
```

## ë§¡ì€ ì—­í•  ë„£ê¸°
## pptì— ì‹œí–‰ì°©ì˜¤ ê³¼ì •, ì¶œë ¥ ê²°ê³¼ ë„£ê¸°(layerë³„ë¡œ, ëª¨ë¸ë³„ë¡œ)
